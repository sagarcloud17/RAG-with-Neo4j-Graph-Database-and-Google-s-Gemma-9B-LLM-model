{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNdic1tvxIZV",
        "outputId": "3255dc40-ff5b-4d45-bbcc-e9b272596c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.5/293.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-community langchain-groq neo4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GraphDatabse Configuration\n",
        "\n",
        "NEO4J_URI=\"xxx\"\n",
        "NEO4J_USERNAME=\"xxx\"\n",
        "NEO4J_PASSWORD=\"xxx\"\n"
      ],
      "metadata": {
        "id": "nTyaWNzxxx7g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"NEO4J_URI\"] = NEO4J_URI\n",
        "os.environ[\"NEO4J_USERNAME\"] = NEO4J_USERNAME\n",
        "os.environ[\"NEO4J_PASSWORD\"] = NEO4J_PASSWORD"
      ],
      "metadata": {
        "id": "QH5mGKTqyI1F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.graphs import Neo4jGraph\n",
        "graph=Neo4jGraph(\n",
        "    url=os.environ.get(\"NEO4J_URI\"),\n",
        "    username=os.environ.get(\"NEO4J_USERNAME\"),\n",
        "    password=os.environ.get(\"NEO4J_PASSWORD\"),\n",
        ")"
      ],
      "metadata": {
        "id": "kjUuwI1wyp0M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaabO6bwy9TD",
        "outputId": "f0d19797-ef8b-4045-91b8-e7198ecf4de1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.graphs.neo4j_graph.Neo4jGraph at 0x7dd79d05ae00>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key=\"xxx\""
      ],
      "metadata": {
        "id": "4tMWH-1azFhU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm=ChatGroq(\n",
        "    groq_api_key=groq_api_key,model_name=\"Gemma2-9b-It\",\n",
        ")\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0393sEvz1t7",
        "outputId": "d61bee12-d07b-4198-e181-8c52e110b94c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7dd76a9a52a0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7dd76a9a5e10>, model_name='Gemma2-9b-It', groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "text = \"\"\"\n",
        "Meta's Llama 3.1 models expand context length to 128K, support eight languages, and include\n",
        "Llama 3.1 405B, the largest open-source AI model.\n",
        "- New features include Llama Guard 3 and Prompt Guard for security and safety.\n",
        "- Over 25 partners, including AWS and NVIDIA, offer services for Llama 3.1.\n",
        "- Llama 3.1 405B enables synthetic data generation and model distillation.\n",
        "\"\"\"\n",
        "documents = [Document(page_content=text)]"
      ],
      "metadata": {
        "id": "4e5QWo1F0bQh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL1_jfdT1HaU",
        "outputId": "d6f1d2ca-a9dd-49fc-d430-af7ca56d580b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=\"\\nMeta's Llama 3.1 models expand context length to 128K, support eight languages, and include\\nLlama 3.1 405B, the largest open-source AI model.\\n- New features include Llama Guard 3 and Prompt Guard for security and safety.\\n- Over 25 partners, including AWS and NVIDIA, offer services for Llama 3.1.\\n- Llama 3.1 405B enables synthetic data generation and model distillation.\\n\")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet langchain_experimental"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_XsBlO31vph",
        "outputId": "4c24e112-5fd4-4487-d062-57b3718ef5e0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/203.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m194.6/203.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
        "\n",
        "llm_transformer=LLMGraphTransformer(llm=llm)"
      ],
      "metadata": {
        "id": "b4CeohHv1NIe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_documents=llm_transformer.convert_to_graph_documents(documents)"
      ],
      "metadata": {
        "id": "Ry5QLyxB1z1C"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBqY7HXf18v7",
        "outputId": "43502b71-a203-4da5-eaa3-2198e62e5116"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[GraphDocument(nodes=[Node(id='Meta', type='Company'), Node(id='Llama 3.1', type='Ai model'), Node(id='Llama 3.1 405B', type='Ai model'), Node(id='Llama Guard 3', type='Security feature'), Node(id='Prompt Guard', type='Safety feature'), Node(id='Aws', type='Company'), Node(id='Nvidia', type='Company'), Node(id='Synthetic Data Generation', type='Feature'), Node(id='Model Distillation', type='Feature')], relationships=[Relationship(source=Node(id='Meta', type='Company'), target=Node(id='Llama 3.1', type='Ai model'), type='DEVELOPS'), Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Llama 3.1 405B', type='Ai model'), type='IS_INSTANCE_OF'), Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Llama Guard 3', type='Security feature'), type='INCLUDES'), Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Prompt Guard', type='Safety feature'), type='INCLUDES'), Relationship(source=Node(id='Llama 3.1 405B', type='Ai model'), target=Node(id='Synthetic Data Generation', type='Feature'), type='ENABLES'), Relationship(source=Node(id='Llama 3.1 405B', type='Ai model'), target=Node(id='Model Distillation', type='Feature'), type='ENABLES'), Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Aws', type='Company'), type='OFFERED_BY'), Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Nvidia', type='Company'), type='OFFERED_BY')], source=Document(page_content=\"\\nMeta's Llama 3.1 models expand context length to 128K, support eight languages, and include\\nLlama 3.1 405B, the largest open-source AI model.\\n- New features include Llama Guard 3 and Prompt Guard for security and safety.\\n- Over 25 partners, including AWS and NVIDIA, offer services for Llama 3.1.\\n- Llama 3.1 405B enables synthetic data generation and model distillation.\\n\"))]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_documents[0].nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arrbBsMN2g4S",
        "outputId": "cebc7971-4017-4fce-bc4f-5f91d733b22a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Node(id='Meta', type='Company'),\n",
              " Node(id='Llama 3.1', type='Ai model'),\n",
              " Node(id='Llama 3.1 405B', type='Ai model'),\n",
              " Node(id='Llama Guard 3', type='Security feature'),\n",
              " Node(id='Prompt Guard', type='Safety feature'),\n",
              " Node(id='Aws', type='Company'),\n",
              " Node(id='Nvidia', type='Company'),\n",
              " Node(id='Synthetic Data Generation', type='Feature'),\n",
              " Node(id='Model Distillation', type='Feature')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_documents[0].relationships"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S06iBLt02llI",
        "outputId": "9f555753-0e57-42b4-c31c-38818d7c658a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Relationship(source=Node(id='Meta', type='Company'), target=Node(id='Llama 3.1', type='Ai model'), type='DEVELOPS'),\n",
              " Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Llama 3.1 405B', type='Ai model'), type='IS_INSTANCE_OF'),\n",
              " Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Llama Guard 3', type='Security feature'), type='INCLUDES'),\n",
              " Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Prompt Guard', type='Safety feature'), type='INCLUDES'),\n",
              " Relationship(source=Node(id='Llama 3.1 405B', type='Ai model'), target=Node(id='Synthetic Data Generation', type='Feature'), type='ENABLES'),\n",
              " Relationship(source=Node(id='Llama 3.1 405B', type='Ai model'), target=Node(id='Model Distillation', type='Feature'), type='ENABLES'),\n",
              " Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Aws', type='Company'), type='OFFERED_BY'),\n",
              " Relationship(source=Node(id='Llama 3.1', type='Ai model'), target=Node(id='Nvidia', type='Company'), type='OFFERED_BY')]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Load the dataset of movie\n",
        "\n",
        "movie_query=\"\"\"\n",
        "LOAD CSV WITH HEADERS FROM\n",
        "'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv' as row\n",
        "\n",
        "MERGE(m:Movie{id:row.movieId})\n",
        "SET m.released = date(row.released),\n",
        "    m.title = row.title,\n",
        "    m.imdbRating = toFloat(row.imdbRating)\n",
        "FOREACH (director in split(row.director, '|') |\n",
        "    MERGE (p:Person {name:trim(director)})\n",
        "    MERGE (p)-[:DIRECTED]->(m))\n",
        "FOREACH (actor in split(row.actors, '|') |\n",
        "    MERGE (p:Person {name:trim(actor)})\n",
        "    MERGE (p)-[:ACTED_IN]->(m))\n",
        "FOREACH (genre in split(row.genres, '|') |\n",
        "    MERGE (g:Genre {name:trim(genre)})\n",
        "    MERGE (m)-[:IN_GENRE]->(g))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VTOHi_iT2w1v"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph.query(movie_query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Jm0gcv6OjC",
        "outputId": "f39d80b7-2a0f-490f-eebf-403afdb2cdfe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph.refresh_schema()\n",
        "print(graph.schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3QSqA5P7ySZ",
        "outputId": "39c2e4d2-9c37-4742-c17f-454527fe0e63"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node properties:\n",
            "Person {name: STRING, age: INTEGER}\n",
            "Movie {id: STRING, released: DATE, title: STRING, imdbRating: FLOAT}\n",
            "Genre {name: STRING}\n",
            "Relationship properties:\n",
            "\n",
            "The relationships:\n",
            "(:Person)-[:DIRECTED]->(:Movie)\n",
            "(:Person)-[:ACTED_IN]->(:Movie)\n",
            "(:Movie)-[:IN_GENRE]->(:Genre)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import GraphCypherQAChain\n",
        "chain=GraphCypherQAChain.from_llm(\n",
        "    llm=llm,\n",
        "    graph=graph,\n",
        "    verbose=True,\n",
        "    #top_k=3,\n",
        ")"
      ],
      "metadata": {
        "id": "uRYp6yzY8wpa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.run(\"Which movies were directed by Peter Jackson?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87tIsP619CZa",
        "outputId": "29ded906-69c9-4490-e9cf-5fbc26835eb4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (p:Person {name: \"Peter Jackson\"})-[:DIRECTED]->(m:Movie) RETURN m.title\n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'m.title': 'Heavenly Creatures'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Heavenly Creatures \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(\"Which movies were directed by Peter Jackson?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz0ILlha9Y5u",
        "outputId": "606535df-04f4-49bc-cdcf-ffb5b35238fd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (p:Person {name: \"Peter Jackson\"})-[:DIRECTED]->(m:Movie) RETURN m.title \n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'m.title': 'Heavenly Creatures'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'query': 'Which movies were directed by Peter Jackson?', 'result': 'Heavenly Creatures \\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.run(\"Director of the movie GoldenEye?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4ZlUY-B9jbj",
        "outputId": "1216b6e9-faa0-4136-8099-1a2302434d14"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
            "Generated Cypher:\n",
            "\u001b[32;1m\u001b[1;3mMATCH (m:Movie {title: \"GoldenEye\"})<-[:DIRECTED]-(p:Person) RETURN p.name  \n",
            "\u001b[0m\n",
            "Full Context:\n",
            "\u001b[32;1m\u001b[1;3m[{'p.name': 'Martin Campbell'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Martin Campbell \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IqmdqYms-od0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}